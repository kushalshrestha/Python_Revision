{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de5c062-a789-408c-8ce0-4e59569e4ab4",
   "metadata": {},
   "source": [
    "# Deeper Dive Into Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791477c-2758-4570-85d9-eb3d5ff28fd8",
   "metadata": {},
   "source": [
    "## Option 1 : Avoid Shuffling and Minimize Data Movement\n",
    "So, what's the alternatives? Here it is:\n",
    "\n",
    "- 1.1 Use `Broadcast Joins` for small tables - when joining a small DataFrame with a large DataFrame, always use broadcast joins to avoid expensive shuffling\n",
    "- 1.2 Use `Bucketing` instead of Shuffling for Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a18ec9-1b1f-4a7a-ae1b-0f99445355ce",
   "metadata": {},
   "source": [
    "### 1.1 Using Broadcast Joins Instead of Shuffling for Joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b329978-380e-4bd6-9768-8c19b9e899a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time (With Broadcast): 0.1299881935119629 seconds\n",
      "Execution Time (Without Broadcast): 0.13207006454467773 seconds\n",
      "----------------NOW WITH NO BROADCAST (Look for (strategy=broadcast))--------------\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [id])\n",
      ":- Relation [id#957,name#958,salary#959,department#960] csv\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- Relation [id#982,bonus_percentage#983] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, name: string, salary: int, department: string, bonus_percentage: double\n",
      "Project [id#957, name#958, salary#959, department#960, bonus_percentage#983]\n",
      "+- Join Inner, (id#957 = id#982)\n",
      "   :- Relation [id#957,name#958,salary#959,department#960] csv\n",
      "   +- ResolvedHint (strategy=broadcast)\n",
      "      +- Relation [id#982,bonus_percentage#983] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#957, name#958, salary#959, department#960, bonus_percentage#983]\n",
      "+- Join Inner, (id#957 = id#982), rightHint=(strategy=broadcast)\n",
      "   :- Filter isnotnull(id#957)\n",
      "   :  +- Relation [id#957,name#958,salary#959,department#960] csv\n",
      "   +- Filter isnotnull(id#982)\n",
      "      +- Relation [id#982,bonus_percentage#983] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#957, name#958, salary#959, department#960, bonus_percentage#983]\n",
      "   +- BroadcastHashJoin [id#957], [id#982], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(id#957)\n",
      "      :  +- FileScan csv [id#957,name#958,salary#959,department#960] Batched: false, DataFilters: [isnotnull(id#957)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string,salary:int,department:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2930]\n",
      "         +- Filter isnotnull(id#982)\n",
      "            +- FileScan csv [id#982,bonus_percentage#983] Batched: false, DataFilters: [isnotnull(id#982)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,bonus_percentage:double>\n",
      "\n",
      "=================================================================================\n",
      "----------------NOW WITH NO BROADCAST (Look for Shuffle Operations)--------------\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [id])\n",
      ":- Relation [id#1022,name#1023,salary#1024,department#1025] csv\n",
      "+- Relation [id#1047,bonus_percentage#1048] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, name: string, salary: int, department: string, bonus_percentage: double\n",
      "Project [id#1022, name#1023, salary#1024, department#1025, bonus_percentage#1048]\n",
      "+- Join Inner, (id#1022 = id#1047)\n",
      "   :- Relation [id#1022,name#1023,salary#1024,department#1025] csv\n",
      "   +- Relation [id#1047,bonus_percentage#1048] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#1022, name#1023, salary#1024, department#1025, bonus_percentage#1048]\n",
      "+- Join Inner, (id#1022 = id#1047)\n",
      "   :- Filter isnotnull(id#1022)\n",
      "   :  +- Relation [id#1022,name#1023,salary#1024,department#1025] csv\n",
      "   +- Filter isnotnull(id#1047)\n",
      "      +- Relation [id#1047,bonus_percentage#1048] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#1022, name#1023, salary#1024, department#1025, bonus_percentage#1048]\n",
      "   +- BroadcastHashJoin [id#1022], [id#1047], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(id#1022)\n",
      "      :  +- FileScan csv [id#1022,name#1023,salary#1024,department#1025] Batched: false, DataFilters: [isnotnull(id#1022)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string,salary:int,department:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2959]\n",
      "         +- Filter isnotnull(id#1047)\n",
      "            +- FileScan csv [id#1047,bonus_percentage#1048] Batched: false, DataFilters: [isnotnull(id#1047)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,bonus_percentage:double>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Broadcast joins for Small Tables\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName('BroadcastJoinExample').getOrCreate()\n",
    "\n",
    "df_large = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema=True)\n",
    "df_small = spark.read.csv('./resources/9_small_file.csv', header=True, inferSchema=True)\n",
    "\n",
    "start_time = time.time()\n",
    "df_joined = df_large.join(broadcast(df_small), 'id')\n",
    "df_joined.count()\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time (With Broadcast): {end_time - start_time} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "# without broadcasting\n",
    "df_large1 = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema=True)\n",
    "df_small1 = spark.read.csv('./resources/9_small_file.csv', header=True, inferSchema=True)\n",
    "\n",
    "start_time = time.time()\n",
    "df_joined1 = df_large1.join(df_small1, 'id')\n",
    "df_joined1.count()\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time (Without Broadcast): {end_time - start_time} seconds\")\n",
    "\n",
    "\n",
    "# use .explain() to view query plans\n",
    "print('----------------NOW WITH NO BROADCAST (Look for (strategy=broadcast))--------------')\n",
    "df_joined.explain(True)\n",
    "print('=================================================================================')\n",
    "print('----------------NOW WITH NO BROADCAST (Look for Shuffle Operations)--------------')\n",
    "df_joined1.explain(True)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7529dd-23b3-4d79-9304-e4e68a619693",
   "metadata": {},
   "source": [
    "### 1.2 Using Bucketing Instead of Shuffling for Joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce25a7c-4a9e-4220-9586-a80d413c7844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
