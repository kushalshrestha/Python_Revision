{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de5c062-a789-408c-8ce0-4e59569e4ab4",
   "metadata": {},
   "source": [
    "# Deeper Dive Into Optimization Techniques\n",
    "\n",
    "## Option 1 : Avoid Shuffling and Minimize Data Movement\n",
    " - 1.1 : Using Broadcast Joins Instead of Shuffling for Joins\n",
    " - 1.2 : Using Bucketing Instead of Shuffling for Joins\n",
    "\n",
    "## Option 2 : Optimize Data Partitioning\n",
    " - 2.1 : Repartition for Parallelism (Be careful too many partitions can increase task scheduling overhead. Also, dataset with 10000 rows is not a large dataset. 10M - 1B is a large dataset\n",
    " - 2.2 : Reduce Partitions for Writing - if you write data as a single file, reduce the number of partitions\n",
    "\n",
    "## Option 3 : Use Efficient Data Formats\n",
    " - 3.1 : Use Parquet Over CSV\n",
    "## Option 4 : Optimize Spark Execution Plan\n",
    "\n",
    "## Option 5 : Use Caching and Persistence Wisely\n",
    "\n",
    "## Option 6 : Skew Handling for Large Datasets\n",
    "\n",
    "## Option 7 : Optimize Joins for Large Datasets\n",
    "\n",
    "## Option 8 : Parallel Processing and UDF Optimization\n",
    "\n",
    "## Option 9 : Optimize Read Performance With Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be0c7a-a19b-44d0-8e23-bd73e78f5643",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1791477c-2758-4570-85d9-eb3d5ff28fd8",
   "metadata": {},
   "source": [
    "## Option 1 : Avoid Shuffling and Minimize Data Movement\n",
    "So, what's the alternatives? Here it is:\n",
    "\n",
    "- 1.1 Use `Broadcast Joins` for small tables - when joining a small DataFrame with a large DataFrame, always use broadcast joins to avoid expensive shuffling\n",
    "- 1.2 Use `Bucketing` instead of Shuffling for Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a18ec9-1b1f-4a7a-ae1b-0f99445355ce",
   "metadata": {},
   "source": [
    "### 1.1 Using Broadcast Joins Instead of Shuffling for Joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b329978-380e-4bd6-9768-8c19b9e899a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time (With Broadcast): 0.1299881935119629 seconds\n",
      "Execution Time (Without Broadcast): 0.13207006454467773 seconds\n",
      "----------------NOW WITH NO BROADCAST (Look for (strategy=broadcast))--------------\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [id])\n",
      ":- Relation [id#957,name#958,salary#959,department#960] csv\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- Relation [id#982,bonus_percentage#983] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, name: string, salary: int, department: string, bonus_percentage: double\n",
      "Project [id#957, name#958, salary#959, department#960, bonus_percentage#983]\n",
      "+- Join Inner, (id#957 = id#982)\n",
      "   :- Relation [id#957,name#958,salary#959,department#960] csv\n",
      "   +- ResolvedHint (strategy=broadcast)\n",
      "      +- Relation [id#982,bonus_percentage#983] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#957, name#958, salary#959, department#960, bonus_percentage#983]\n",
      "+- Join Inner, (id#957 = id#982), rightHint=(strategy=broadcast)\n",
      "   :- Filter isnotnull(id#957)\n",
      "   :  +- Relation [id#957,name#958,salary#959,department#960] csv\n",
      "   +- Filter isnotnull(id#982)\n",
      "      +- Relation [id#982,bonus_percentage#983] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#957, name#958, salary#959, department#960, bonus_percentage#983]\n",
      "   +- BroadcastHashJoin [id#957], [id#982], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(id#957)\n",
      "      :  +- FileScan csv [id#957,name#958,salary#959,department#960] Batched: false, DataFilters: [isnotnull(id#957)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string,salary:int,department:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2930]\n",
      "         +- Filter isnotnull(id#982)\n",
      "            +- FileScan csv [id#982,bonus_percentage#983] Batched: false, DataFilters: [isnotnull(id#982)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,bonus_percentage:double>\n",
      "\n",
      "=================================================================================\n",
      "----------------NOW WITH NO BROADCAST (Look for Shuffle Operations)--------------\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [id])\n",
      ":- Relation [id#1022,name#1023,salary#1024,department#1025] csv\n",
      "+- Relation [id#1047,bonus_percentage#1048] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, name: string, salary: int, department: string, bonus_percentage: double\n",
      "Project [id#1022, name#1023, salary#1024, department#1025, bonus_percentage#1048]\n",
      "+- Join Inner, (id#1022 = id#1047)\n",
      "   :- Relation [id#1022,name#1023,salary#1024,department#1025] csv\n",
      "   +- Relation [id#1047,bonus_percentage#1048] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#1022, name#1023, salary#1024, department#1025, bonus_percentage#1048]\n",
      "+- Join Inner, (id#1022 = id#1047)\n",
      "   :- Filter isnotnull(id#1022)\n",
      "   :  +- Relation [id#1022,name#1023,salary#1024,department#1025] csv\n",
      "   +- Filter isnotnull(id#1047)\n",
      "      +- Relation [id#1047,bonus_percentage#1048] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#1022, name#1023, salary#1024, department#1025, bonus_percentage#1048]\n",
      "   +- BroadcastHashJoin [id#1022], [id#1047], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(id#1022)\n",
      "      :  +- FileScan csv [id#1022,name#1023,salary#1024,department#1025] Batched: false, DataFilters: [isnotnull(id#1022)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string,salary:int,department:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2959]\n",
      "         +- Filter isnotnull(id#1047)\n",
      "            +- FileScan csv [id#1047,bonus_percentage#1048] Batched: false, DataFilters: [isnotnull(id#1047)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,bonus_percentage:double>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Broadcast joins for Small Tables\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName('BroadcastJoinExample').getOrCreate()\n",
    "\n",
    "df_large = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema=True)\n",
    "df_small = spark.read.csv('./resources/9_small_file.csv', header=True, inferSchema=True)\n",
    "\n",
    "start_time = time.time()\n",
    "df_joined = df_large.join(broadcast(df_small), 'id')\n",
    "df_joined.count()\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time (With Broadcast): {end_time - start_time} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "# without broadcasting\n",
    "df_large1 = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema=True)\n",
    "df_small1 = spark.read.csv('./resources/9_small_file.csv', header=True, inferSchema=True)\n",
    "\n",
    "start_time = time.time()\n",
    "df_joined1 = df_large1.join(df_small1, 'id')\n",
    "df_joined1.count()\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time (Without Broadcast): {end_time - start_time} seconds\")\n",
    "\n",
    "\n",
    "# use .explain() to view query plans\n",
    "print('----------------NOW WITH NO BROADCAST (Look for (strategy=broadcast))--------------')\n",
    "df_joined.explain(True)\n",
    "print('=================================================================================')\n",
    "print('----------------NOW WITH NO BROADCAST (Look for Shuffle Operations)--------------')\n",
    "df_joined1.explain(True)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7529dd-23b3-4d79-9304-e4e68a619693",
   "metadata": {},
   "source": [
    "### 1.2 Using Bucketing Instead of Shuffling for Joins\n",
    "\n",
    "- `If you're frequently using same column for join`, use bucketing to pre-sort the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ce25a7c-4a9e-4220-9586-a80d413c7844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time 0.11 seconds\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [id])\n",
      ":- Relation [id#1435,name#1436,salary#1437,department#1438] csv\n",
      "+- Relation [id#1460,bonus_percentage#1461] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, name: string, salary: int, department: string, bonus_percentage: double\n",
      "Project [id#1435, name#1436, salary#1437, department#1438, bonus_percentage#1461]\n",
      "+- Join Inner, (id#1435 = id#1460)\n",
      "   :- Relation [id#1435,name#1436,salary#1437,department#1438] csv\n",
      "   +- Relation [id#1460,bonus_percentage#1461] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#1435, name#1436, salary#1437, department#1438, bonus_percentage#1461]\n",
      "+- Join Inner, (id#1435 = id#1460)\n",
      "   :- Filter isnotnull(id#1435)\n",
      "   :  +- Relation [id#1435,name#1436,salary#1437,department#1438] csv\n",
      "   +- Filter isnotnull(id#1460)\n",
      "      +- Relation [id#1460,bonus_percentage#1461] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#1435, name#1436, salary#1437, department#1438, bonus_percentage#1461]\n",
      "   +- BroadcastHashJoin [id#1435], [id#1460], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(id#1435)\n",
      "      :  +- FileScan csv [id#1435,name#1436,salary#1437,department#1438] Batched: false, DataFilters: [isnotnull(id#1435)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string,salary:int,department:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4150]\n",
      "         +- Filter isnotnull(id#1460)\n",
      "            +- FileScan csv [id#1460,bonus_percentage#1461] Batched: false, DataFilters: [isnotnull(id#1460)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,bonus_percentage:double>\n",
      "\n",
      "Execution Time 0.08 seconds\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [id])\n",
      ":- SubqueryAlias spark_catalog.default.large_bucketed_table\n",
      ":  +- Relation spark_catalog.default.large_bucketed_table[id#1501,name#1502,salary#1503,department#1504] parquet\n",
      "+- SubqueryAlias spark_catalog.default.small_bucketed_table\n",
      "   +- Relation spark_catalog.default.small_bucketed_table[id#1509,bonus_percentage#1510] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, name: string, salary: int, department: string, bonus_percentage: double\n",
      "Project [id#1501, name#1502, salary#1503, department#1504, bonus_percentage#1510]\n",
      "+- Join Inner, (id#1501 = id#1509)\n",
      "   :- SubqueryAlias spark_catalog.default.large_bucketed_table\n",
      "   :  +- Relation spark_catalog.default.large_bucketed_table[id#1501,name#1502,salary#1503,department#1504] parquet\n",
      "   +- SubqueryAlias spark_catalog.default.small_bucketed_table\n",
      "      +- Relation spark_catalog.default.small_bucketed_table[id#1509,bonus_percentage#1510] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#1501, name#1502, salary#1503, department#1504, bonus_percentage#1510]\n",
      "+- Join Inner, (id#1501 = id#1509)\n",
      "   :- Filter isnotnull(id#1501)\n",
      "   :  +- Relation spark_catalog.default.large_bucketed_table[id#1501,name#1502,salary#1503,department#1504] parquet\n",
      "   +- Filter isnotnull(id#1509)\n",
      "      +- Relation spark_catalog.default.small_bucketed_table[id#1509,bonus_percentage#1510] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#1501, name#1502, salary#1503, department#1504, bonus_percentage#1510]\n",
      "   +- BroadcastHashJoin [id#1501], [id#1509], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(id#1501)\n",
      "      :  +- FileScan parquet spark_catalog.default.large_bucketed_table[id#1501,name#1502,salary#1503,department#1504] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [isnotnull(id#1501)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98.%20PyS..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string,salary:int,department:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4323]\n",
      "         +- Filter isnotnull(id#1509)\n",
      "            +- FileScan parquet spark_catalog.default.small_bucketed_table[id#1509,bonus_percentage#1510] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [isnotnull(id#1509)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98.%20PyS..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,bonus_percentage:double>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName('BucketOptimization').getOrCreate()\n",
    "\n",
    "df_large = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema=True)\n",
    "df_small = spark.read.csv('./resources/9_small_file.csv', header=True, inferSchema=True)\n",
    "\n",
    "# apply bucketing and sorting (avoids shuffling in joins)\n",
    "df_large.write.mode('overwrite').bucketBy(8, 'id').sortBy('id').saveAsTable('large_bucketed_table')\n",
    "df_small.write.mode('overwrite').bucketBy(8, 'id').sortBy('id').saveAsTable('small_bucketed_table')\n",
    "\n",
    "# Now perform joins (Shuffle Heavy)\n",
    "start_time = time.time()\n",
    "df_joined = df_large.join(df_small, 'id')\n",
    "df_joined.count()\n",
    "end_time = time.time()\n",
    "print(f'Execution Time {end_time - start_time:.2f} seconds')\n",
    "df_joined.explain(True)\n",
    "\n",
    "# Now perform joins (No Shuffle) - Optimized join using Bucketed tables\n",
    "df_large_bucketed = spark.table('large_bucketed_table')\n",
    "df_small_bucketed = spark.table('small_bucketed_table')\n",
    "start_time = time.time()\n",
    "df_joined = df_large_bucketed.join(df_small_bucketed, 'id')\n",
    "df_joined.count()\n",
    "end_time = time.time()\n",
    "print(f'Execution Time {end_time - start_time:.2f} seconds')\n",
    "df_joined.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0317a4-4aac-4746-9417-5db92f9a3d9b",
   "metadata": {},
   "source": [
    "# Option 2 : Optimize Data Partioning\n",
    "\n",
    "### 2.1 : Repartition for Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fae6a307-68d6-4914-b07e-4fc67ad23009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time (for Default Partitions) : 0.07 seconds\n",
      "Execution Time (for Repartitioned) : 0.09 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName('RepartitionOptimization').getOrCreate()\n",
    "\n",
    "df_large = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema=True)\n",
    "# Spark automatically assigns a default number of partitions\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "df_large.count()\n",
    "end_time = time.time()\n",
    "print(f'Execution Time (for Default Partitions) : {end_time - start_time:.2f} seconds')\n",
    "\n",
    "# Now, increase partitions for paralled processing\n",
    "df_repartitioned = df_large.repartition(10) # Increase to 10 partitions\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "df_repartitioned.count()\n",
    "end_time = time.time()\n",
    "print(f'Execution Time (for Repartitioned) : {end_time - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27919b99-0cef-424e-9c4a-e9efd40b1fba",
   "metadata": {},
   "source": [
    "### 2.2 : Reduce Partitions for Writing Using coalesce()\n",
    "If you write data as a single file, reduce the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2e45596-c50b-4894-b2b2-87be76034ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial partitions: 1\n",
      "Execution Time (for Repartitioned) : 0.25 seconds\n",
      "Partitions after coalesce: 1\n",
      "Execution Time (for coalesced - reducing to 1 partition) : 0.16 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('ReducePartitionsForWriting').getOrCreate()\n",
    "\n",
    "df_large = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema = True)\n",
    "\n",
    "start_time = time.time()\n",
    "df_repartitioned = df_large.repartition(10) # Increase to 10 partitions\n",
    "df_repartitioned.write.mode('overwrite').parquet('./output/writeWith10Partitions')\n",
    "end_time = time.time()\n",
    "print(f'Initial partitions: {df_large.rdd.getNumPartitions()}')\n",
    "print(f'Execution Time (for Repartitioned) : {end_time - start_time:.2f} seconds')\n",
    "\n",
    "\n",
    "# Reduce partitions using coalesce()\n",
    "start_time = time.time()\n",
    "df_coalesced = df_large.coalesce(1) # reducing to 1 partition\n",
    "df_coalesced.write.mode('overwrite').parquet('./output/writeWith1Partition')\n",
    "end_time = time.time()\n",
    "print(f'Partitions after coalesce: {df_large.rdd.getNumPartitions()}')\n",
    "print(f'Execution Time (for coalesced - reducing to 1 partition) : {end_time - start_time:.2f} seconds')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
