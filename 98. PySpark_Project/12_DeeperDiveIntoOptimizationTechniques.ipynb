{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de5c062-a789-408c-8ce0-4e59569e4ab4",
   "metadata": {},
   "source": [
    "# Deeper Dive Into Optimization Techniques\n",
    "\n",
    "## Option 1 : Avoid Shuffling and Minimize Data Movement\n",
    " - 1.1 : Using Broadcast Joins Instead of Shuffling for Joins\n",
    " - 1.2 : Using Bucketing Instead of Shuffling for Joins\n",
    "\n",
    "## Option 2 : Optimize Data Partitioning\n",
    " - 2.1 : Repartition for Parallelism (Be careful too many partitions can increase task scheduling overhead. Also, dataset with 10000 rows is not a large dataset. 10M - 1B is a large dataset\n",
    " - 2.2 : Reduce Partitions for Writing - if you write data as a single file, reduce the number of partitions\n",
    "\n",
    "## Option 3 : Use Efficient Data Formats\n",
    " - 3.1 : Use Parquet Over CSV\n",
    "   \n",
    " `df.write.parquet('output.parquet')`\n",
    "\n",
    " * Columnar Storage - Parquet stores data column-wise resulting to faster queries\n",
    " * Efficient compression - Parquet compresses data efficiently using advanced compression techniques like Snappy, Gzip, LZ4\n",
    "## Option 4 : Optimize Spark Execution Plan\n",
    "\n",
    "`df.join(df2, 'id').explain(True)`\n",
    "\n",
    "* Look for expensive \"Shuffle Exchange\" operations\n",
    "\n",
    "## Option 5 : Use Caching and Persistence Wisely\n",
    "\n",
    "`df.cache()`\n",
    "\n",
    "* Cache the data - if you reuse dataframe multiple times. But, don't cache huge dataframes unless necessary\n",
    "\n",
    "  `df.persist(StorageLevel.DISK_ONLY`\n",
    "\n",
    "  - PERSIST = Stores in memory + disk\n",
    "  - If memory is limited, use disk persistence\n",
    "\n",
    "## Option 6 : Skew Handling for Large Datasets\n",
    "\n",
    "Step 1 : Check Data Skew - check if some partitions are much larger than others\n",
    "Step 2 : Perform Salting - if one key has too many records, introduce salting b/c we need to prevent one partition from handling most of the data\n",
    "\n",
    "## Option 7 : Optimize Joins for Large Datasets\n",
    "\n",
    "- Use Sort-Merge Join Instead of Shuffle-Hash Join (If both dataframes are too large for memory, sort them first)\n",
    "\n",
    "`df_large = df_large.sort('id')`\n",
    "\n",
    "`df_small = df_small.sort('id')`\n",
    "\n",
    "`df_joined = df_large.join(df_small, 'id')`\n",
    "\n",
    "## Option 8 : Parallel Processing and UDF Optimization\n",
    "\n",
    "## Option 9 : Optimize Read Performance With Indexing\n",
    "- Optimizing by using Z-Ordering on large tables for faster queries. WHY? Sottres similar rows together, reducing scan time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791477c-2758-4570-85d9-eb3d5ff28fd8",
   "metadata": {},
   "source": [
    "## Option 1 : Avoid Shuffling and Minimize Data Movement\n",
    "So, what's the alternatives? Here it is:\n",
    "\n",
    "- 1.1 Use `Broadcast Joins` for small tables - when joining a small DataFrame with a large DataFrame, always use broadcast joins to avoid expensive shuffling\n",
    "- 1.2 Use `Bucketing` instead of Shuffling for Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a18ec9-1b1f-4a7a-ae1b-0f99445355ce",
   "metadata": {},
   "source": [
    "### 1.1 Using Broadcast Joins Instead of Shuffling for Joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b329978-380e-4bd6-9768-8c19b9e899a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/03/29 21:45:54 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time (With Broadcast): 0.1299881935119629 seconds\n",
      "Execution Time (Without Broadcast): 0.13207006454467773 seconds\n",
      "----------------NOW WITH NO BROADCAST (Look for (strategy=broadcast))--------------\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [id])\n",
      ":- Relation [id#957,name#958,salary#959,department#960] csv\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- Relation [id#982,bonus_percentage#983] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, name: string, salary: int, department: string, bonus_percentage: double\n",
      "Project [id#957, name#958, salary#959, department#960, bonus_percentage#983]\n",
      "+- Join Inner, (id#957 = id#982)\n",
      "   :- Relation [id#957,name#958,salary#959,department#960] csv\n",
      "   +- ResolvedHint (strategy=broadcast)\n",
      "      +- Relation [id#982,bonus_percentage#983] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#957, name#958, salary#959, department#960, bonus_percentage#983]\n",
      "+- Join Inner, (id#957 = id#982), rightHint=(strategy=broadcast)\n",
      "   :- Filter isnotnull(id#957)\n",
      "   :  +- Relation [id#957,name#958,salary#959,department#960] csv\n",
      "   +- Filter isnotnull(id#982)\n",
      "      +- Relation [id#982,bonus_percentage#983] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#957, name#958, salary#959, department#960, bonus_percentage#983]\n",
      "   +- BroadcastHashJoin [id#957], [id#982], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(id#957)\n",
      "      :  +- FileScan csv [id#957,name#958,salary#959,department#960] Batched: false, DataFilters: [isnotnull(id#957)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string,salary:int,department:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2930]\n",
      "         +- Filter isnotnull(id#982)\n",
      "            +- FileScan csv [id#982,bonus_percentage#983] Batched: false, DataFilters: [isnotnull(id#982)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,bonus_percentage:double>\n",
      "\n",
      "=================================================================================\n",
      "----------------NOW WITH NO BROADCAST (Look for Shuffle Operations)--------------\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [id])\n",
      ":- Relation [id#1022,name#1023,salary#1024,department#1025] csv\n",
      "+- Relation [id#1047,bonus_percentage#1048] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, name: string, salary: int, department: string, bonus_percentage: double\n",
      "Project [id#1022, name#1023, salary#1024, department#1025, bonus_percentage#1048]\n",
      "+- Join Inner, (id#1022 = id#1047)\n",
      "   :- Relation [id#1022,name#1023,salary#1024,department#1025] csv\n",
      "   +- Relation [id#1047,bonus_percentage#1048] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#1022, name#1023, salary#1024, department#1025, bonus_percentage#1048]\n",
      "+- Join Inner, (id#1022 = id#1047)\n",
      "   :- Filter isnotnull(id#1022)\n",
      "   :  +- Relation [id#1022,name#1023,salary#1024,department#1025] csv\n",
      "   +- Filter isnotnull(id#1047)\n",
      "      +- Relation [id#1047,bonus_percentage#1048] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#1022, name#1023, salary#1024, department#1025, bonus_percentage#1048]\n",
      "   +- BroadcastHashJoin [id#1022], [id#1047], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(id#1022)\n",
      "      :  +- FileScan csv [id#1022,name#1023,salary#1024,department#1025] Batched: false, DataFilters: [isnotnull(id#1022)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string,salary:int,department:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2959]\n",
      "         +- Filter isnotnull(id#1047)\n",
      "            +- FileScan csv [id#1047,bonus_percentage#1048] Batched: false, DataFilters: [isnotnull(id#1047)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,bonus_percentage:double>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Broadcast joins for Small Tables\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName('BroadcastJoinExample').getOrCreate()\n",
    "\n",
    "df_large = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema=True)\n",
    "df_small = spark.read.csv('./resources/9_small_file.csv', header=True, inferSchema=True)\n",
    "\n",
    "start_time = time.time()\n",
    "df_joined = df_large.join(broadcast(df_small), 'id')\n",
    "df_joined.count()\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time (With Broadcast): {end_time - start_time} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "# without broadcasting\n",
    "df_large1 = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema=True)\n",
    "df_small1 = spark.read.csv('./resources/9_small_file.csv', header=True, inferSchema=True)\n",
    "\n",
    "start_time = time.time()\n",
    "df_joined1 = df_large1.join(df_small1, 'id')\n",
    "df_joined1.count()\n",
    "end_time = time.time()\n",
    "print(f\"Execution Time (Without Broadcast): {end_time - start_time} seconds\")\n",
    "\n",
    "\n",
    "# use .explain() to view query plans\n",
    "print('----------------NOW WITH NO BROADCAST (Look for (strategy=broadcast))--------------')\n",
    "df_joined.explain(True)\n",
    "print('=================================================================================')\n",
    "print('----------------NOW WITH NO BROADCAST (Look for Shuffle Operations)--------------')\n",
    "df_joined1.explain(True)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7529dd-23b3-4d79-9304-e4e68a619693",
   "metadata": {},
   "source": [
    "### 1.2 Using Bucketing Instead of Shuffling for Joins\n",
    "\n",
    "- `If you're frequently using same column for join`, use bucketing to pre-sort the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ce25a7c-4a9e-4220-9586-a80d413c7844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time 0.11 seconds\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [id])\n",
      ":- Relation [id#1435,name#1436,salary#1437,department#1438] csv\n",
      "+- Relation [id#1460,bonus_percentage#1461] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, name: string, salary: int, department: string, bonus_percentage: double\n",
      "Project [id#1435, name#1436, salary#1437, department#1438, bonus_percentage#1461]\n",
      "+- Join Inner, (id#1435 = id#1460)\n",
      "   :- Relation [id#1435,name#1436,salary#1437,department#1438] csv\n",
      "   +- Relation [id#1460,bonus_percentage#1461] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#1435, name#1436, salary#1437, department#1438, bonus_percentage#1461]\n",
      "+- Join Inner, (id#1435 = id#1460)\n",
      "   :- Filter isnotnull(id#1435)\n",
      "   :  +- Relation [id#1435,name#1436,salary#1437,department#1438] csv\n",
      "   +- Filter isnotnull(id#1460)\n",
      "      +- Relation [id#1460,bonus_percentage#1461] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#1435, name#1436, salary#1437, department#1438, bonus_percentage#1461]\n",
      "   +- BroadcastHashJoin [id#1435], [id#1460], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(id#1435)\n",
      "      :  +- FileScan csv [id#1435,name#1436,salary#1437,department#1438] Batched: false, DataFilters: [isnotnull(id#1435)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string,salary:int,department:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4150]\n",
      "         +- Filter isnotnull(id#1460)\n",
      "            +- FileScan csv [id#1460,bonus_percentage#1461] Batched: false, DataFilters: [isnotnull(id#1460)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,bonus_percentage:double>\n",
      "\n",
      "Execution Time 0.08 seconds\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [id])\n",
      ":- SubqueryAlias spark_catalog.default.large_bucketed_table\n",
      ":  +- Relation spark_catalog.default.large_bucketed_table[id#1501,name#1502,salary#1503,department#1504] parquet\n",
      "+- SubqueryAlias spark_catalog.default.small_bucketed_table\n",
      "   +- Relation spark_catalog.default.small_bucketed_table[id#1509,bonus_percentage#1510] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: int, name: string, salary: int, department: string, bonus_percentage: double\n",
      "Project [id#1501, name#1502, salary#1503, department#1504, bonus_percentage#1510]\n",
      "+- Join Inner, (id#1501 = id#1509)\n",
      "   :- SubqueryAlias spark_catalog.default.large_bucketed_table\n",
      "   :  +- Relation spark_catalog.default.large_bucketed_table[id#1501,name#1502,salary#1503,department#1504] parquet\n",
      "   +- SubqueryAlias spark_catalog.default.small_bucketed_table\n",
      "      +- Relation spark_catalog.default.small_bucketed_table[id#1509,bonus_percentage#1510] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [id#1501, name#1502, salary#1503, department#1504, bonus_percentage#1510]\n",
      "+- Join Inner, (id#1501 = id#1509)\n",
      "   :- Filter isnotnull(id#1501)\n",
      "   :  +- Relation spark_catalog.default.large_bucketed_table[id#1501,name#1502,salary#1503,department#1504] parquet\n",
      "   +- Filter isnotnull(id#1509)\n",
      "      +- Relation spark_catalog.default.small_bucketed_table[id#1509,bonus_percentage#1510] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#1501, name#1502, salary#1503, department#1504, bonus_percentage#1510]\n",
      "   +- BroadcastHashJoin [id#1501], [id#1509], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(id#1501)\n",
      "      :  +- FileScan parquet spark_catalog.default.large_bucketed_table[id#1501,name#1502,salary#1503,department#1504] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [isnotnull(id#1501)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98.%20PyS..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,name:string,salary:int,department:string>\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=4323]\n",
      "         +- Filter isnotnull(id#1509)\n",
      "            +- FileScan parquet spark_catalog.default.small_bucketed_table[id#1509,bonus_percentage#1510] Batched: true, Bucketed: false (disabled by query planner), DataFilters: [isnotnull(id#1509)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98.%20PyS..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:int,bonus_percentage:double>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName('BucketOptimization').getOrCreate()\n",
    "\n",
    "df_large = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema=True)\n",
    "df_small = spark.read.csv('./resources/9_small_file.csv', header=True, inferSchema=True)\n",
    "\n",
    "# apply bucketing and sorting (avoids shuffling in joins)\n",
    "df_large.write.mode('overwrite').bucketBy(8, 'id').sortBy('id').saveAsTable('large_bucketed_table')\n",
    "df_small.write.mode('overwrite').bucketBy(8, 'id').sortBy('id').saveAsTable('small_bucketed_table')\n",
    "\n",
    "# Now perform joins (Shuffle Heavy)\n",
    "start_time = time.time()\n",
    "df_joined = df_large.join(df_small, 'id')\n",
    "df_joined.count()\n",
    "end_time = time.time()\n",
    "print(f'Execution Time {end_time - start_time:.2f} seconds')\n",
    "df_joined.explain(True)\n",
    "\n",
    "# Now perform joins (No Shuffle) - Optimized join using Bucketed tables\n",
    "df_large_bucketed = spark.table('large_bucketed_table')\n",
    "df_small_bucketed = spark.table('small_bucketed_table')\n",
    "start_time = time.time()\n",
    "df_joined = df_large_bucketed.join(df_small_bucketed, 'id')\n",
    "df_joined.count()\n",
    "end_time = time.time()\n",
    "print(f'Execution Time {end_time - start_time:.2f} seconds')\n",
    "df_joined.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0317a4-4aac-4746-9417-5db92f9a3d9b",
   "metadata": {},
   "source": [
    "# Option 2 : Optimize Data Partioning\n",
    "\n",
    "### 2.1 : Repartition for Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fae6a307-68d6-4914-b07e-4fc67ad23009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time (for Default Partitions) : 0.07 seconds\n",
      "Execution Time (for Repartitioned) : 0.09 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder.appName('RepartitionOptimization').getOrCreate()\n",
    "\n",
    "df_large = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema=True)\n",
    "# Spark automatically assigns a default number of partitions\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "df_large.count()\n",
    "end_time = time.time()\n",
    "print(f'Execution Time (for Default Partitions) : {end_time - start_time:.2f} seconds')\n",
    "\n",
    "# Now, increase partitions for paralled processing\n",
    "df_repartitioned = df_large.repartition(10) # Increase to 10 partitions\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "df_repartitioned.count()\n",
    "end_time = time.time()\n",
    "print(f'Execution Time (for Repartitioned) : {end_time - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27919b99-0cef-424e-9c4a-e9efd40b1fba",
   "metadata": {},
   "source": [
    "### 2.2 : Reduce Partitions for Writing Using coalesce()\n",
    "If you write data as a single file, reduce the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2e45596-c50b-4894-b2b2-87be76034ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial partitions: 1\n",
      "Execution Time (for Repartitioned) : 0.25 seconds\n",
      "Partitions after coalesce: 1\n",
      "Execution Time (for coalesced - reducing to 1 partition) : 0.16 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('ReducePartitionsForWriting').getOrCreate()\n",
    "\n",
    "df_large = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema = True)\n",
    "\n",
    "start_time = time.time()\n",
    "df_repartitioned = df_large.repartition(10) # Increase to 10 partitions\n",
    "df_repartitioned.write.mode('overwrite').parquet('./output/writeWith10Partitions')\n",
    "end_time = time.time()\n",
    "print(f'Initial partitions: {df_large.rdd.getNumPartitions()}')\n",
    "print(f'Execution Time (for Repartitioned) : {end_time - start_time:.2f} seconds')\n",
    "\n",
    "\n",
    "# Reduce partitions using coalesce()\n",
    "start_time = time.time()\n",
    "df_coalesced = df_large.coalesce(1) # reducing to 1 partition\n",
    "df_coalesced.write.mode('overwrite').parquet('./output/writeWith1Partition')\n",
    "end_time = time.time()\n",
    "print(f'Partitions after coalesce: {df_large.rdd.getNumPartitions()}')\n",
    "print(f'Execution Time (for coalesced - reducing to 1 partition) : {end_time - start_time:.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d41160-32d0-4e18-bb57-6ca554a65577",
   "metadata": {},
   "source": [
    "### Skew Handling for Large DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ac4b410-3d56-4a27-ba20-0d64c5d96583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Skewness\n",
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|   Finance|25170|\n",
      "|        HR|25033|\n",
      "|     Sales|24901|\n",
      "|        IT|24896|\n",
      "+----------+-----+\n",
      "\n",
      "After Applying Salting:\n",
      "+----------+---------------+-----+\n",
      "|department|salt_department|count|\n",
      "+----------+---------------+-----+\n",
      "|   Finance|              7| 2573|\n",
      "|   Finance|              1| 2568|\n",
      "|     Sales|              9| 2564|\n",
      "|        HR|              8| 2560|\n",
      "|        HR|              6| 2552|\n",
      "|        HR|              7| 2543|\n",
      "|        IT|              1| 2538|\n",
      "|        IT|              8| 2537|\n",
      "|        IT|              5| 2530|\n",
      "|   Finance|              2| 2525|\n",
      "|        HR|              2| 2520|\n",
      "|   Finance|              8| 2520|\n",
      "|     Sales|              7| 2517|\n",
      "|     Sales|              0| 2515|\n",
      "|   Finance|              9| 2511|\n",
      "|        HR|              0| 2508|\n",
      "|   Finance|              0| 2507|\n",
      "|        HR|              1| 2506|\n",
      "|     Sales|              6| 2506|\n",
      "|        IT|              4| 2503|\n",
      "+----------+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand\n",
    "\n",
    "spark = SparkSession.builder.appName('HandleDataSkew').getOrCreate()\n",
    "\n",
    "df_large = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema=True)\n",
    "df_skew_check = df_large.groupBy('department').count().orderBy(col('count').desc())\n",
    "print('Checking Skewness')\n",
    "df_skew_check.show()\n",
    "\n",
    "\n",
    "# Applying salting to reduce skewness\n",
    "df_large = df_large.withColumn('salt_department', (rand() * 10).cast('int'))\n",
    "df_salted = df_large.repartition('department', 'salt_department') # Repartition using both 'department' and 'salt_department'\n",
    "df_salted_skew_check = df_salted.groupBy('department', 'salt_department').count().orderBy(col('count').desc())\n",
    "print('After Applying Salting:')\n",
    "df_salted_skew_check.show()\n",
    "\n",
    "# Now dropping salt column\n",
    "df_final = df_salted.drop('salt_department')\n",
    "\n",
    "# stopping Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64a18e-ac9b-4221-be9f-61b4b9e55118",
   "metadata": {},
   "source": [
    "# Option 8 : Parallel Processing and UDF Optimization\n",
    "- Avoid standard Python UDFs (slow) and use vectorized UDFs(pandas_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b4f99270-ea21-4776-ab37-ac4a0241943b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+----------+-------+\n",
      "| id|       name|salary|department|  bonus|\n",
      "+---+-----------+------+----------+-------+\n",
      "|  1| Employee_1|113419|   Finance|11341.9|\n",
      "|  2| Employee_2| 44395|   Finance| 4439.5|\n",
      "|  3| Employee_3|134158|     Sales|13415.8|\n",
      "|  4| Employee_4| 59648|     Sales| 5964.8|\n",
      "|  5| Employee_5| 96002|   Finance| 9600.2|\n",
      "|  6| Employee_6| 69460|     Sales| 6946.0|\n",
      "|  7| Employee_7| 39196|   Finance| 3919.6|\n",
      "|  8| Employee_8|132461|   Finance|13246.1|\n",
      "|  9| Employee_9|111085|     Sales|11108.5|\n",
      "| 10|Employee_10| 93008|   Finance| 9300.8|\n",
      "+---+-----------+------+----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName('PandasUDFExample').getOrCreate()\n",
    "df_large = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema = True)\n",
    "\n",
    "# Define Pandas UDF for Bonus Calculation\n",
    "@pandas_udf(FloatType())\n",
    "def calculate_bonus(salary: pd.Series) -> pd.Series:\n",
    "    return salary * 0.1\n",
    "\n",
    "df_large = df_large.withColumn('bonus', calculate_bonus(df_large['salary']))\n",
    "df_large.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873032fb-6a2c-47e5-ba93-681f1916d17b",
   "metadata": {},
   "source": [
    "## Optimize Read Performance With Indexing\n",
    "\n",
    "### 1. Use Data Skipping with Z-Ordering (if specific columns is used for filtering)\n",
    "- For faster queries on large tables, use Z-Ordering (especially on Databricks) b/c it stores similar rows together, reducing scan time\n",
    "\n",
    "## Open Table Formats (https://spark.apache.org/third-party-projects.html) : Delta Lake, Hudi, Apache Iceberg\n",
    "### Delta Lake - Storage layer that provides ACID transactions and scalable metadata handling for Apache Spark workloads\n",
    "* An open-source storage framework that enables building a format agnostic Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, Hive, Snowflake, Google BigQuery, Athena, Redshift, Databricks, Azure Fabric and APIs for Scala, Java, Rust, and Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b848ccf9-8c71-4173-9a0e-4241f4c6dd81",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1345.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m df_large = spark.read.csv(\u001b[33m'\u001b[39m\u001b[33m./resources/8_large_file.csv\u001b[39m\u001b[33m'\u001b[39m, header=\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Write the DataFrame to Delta format (Parquet-based storage)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mdf_large\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdelta\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moptimizeWrite\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./output/z_order_optimization_table\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m df_large.write.csv(\u001b[33m'\u001b[39m\u001b[33m./output/without_z_order_optimization_mode.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Z-ordering - after writing the data we perform Z-Ordering optimization on the `department` column by calling, so that queries that filter by `department` will benefit from faster data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1463\u001b[39m, in \u001b[36mDataFrameWriter.save\u001b[39m\u001b[34m(self, path, format, mode, partitionBy, **options)\u001b[39m\n\u001b[32m   1461\u001b[39m     \u001b[38;5;28mself\u001b[39m._jwrite.save()\n\u001b[32m   1462\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1463\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1345.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# builder = pyspark.sql.SparkSession.builder.appName(\"ZOrderingOptimization\").config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\").config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\").getOrCreate()\n",
    "\n",
    "# spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.appName('ZOrderingOptimization').config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\").getOrCreate()\n",
    "\n",
    "df_large = spark.read.csv('./resources/8_large_file.csv', header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Write the DataFrame to Delta format (Parquet-based storage)\n",
    "df_large.write.mode('overwrite').format('delta').option('optimizeWrite', 'true').save('./output/z_order_optimization_table')\n",
    "df_large.write.csv('./output/without_z_order_optimization_mode.csv')\n",
    "\n",
    "# Z-ordering - after writing the data we perform Z-Ordering optimization on the `department` column by calling, so that queries that filter by `department` will benefit from faster data\n",
    "spark.sql('OPTIMIZE delta.`./output/z_order_optimization_table` ZORDER BY (department)')\n",
    "\n",
    "# Now you can run efficient queries on this table. Example:\n",
    "start_time = time.time()\n",
    "df_optimized = spark.read.format('delta').load('./output/z_order_optimization_table')\n",
    "df_optimized.count()\n",
    "end_time = time.time()\n",
    "print(f'Execution Time (Reading Z-ordered dataframe) : {end_time - start_time:.2f} seconds')\n",
    "\n",
    "start_time = time.time()\n",
    "df_non_optimized = spark.read.csv('./output/without_z_order_optimization_mode.csv')\n",
    "df_non_optimized.count()\n",
    "end_time = time.time()\n",
    "print(f'Execution Time (Reading Non Z-ordered dataframe) : {end_time - start_time:.2f} seconds')\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e695950-57c6-49be-b72c-79b4f21d945b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.5'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "22bdf7af-ba3a-4aed-9855-9306e7d2d124",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1302965558.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpyspark --packages io.delta:delta-core_2.12:2.1.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\"\u001b[39m\n                       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
