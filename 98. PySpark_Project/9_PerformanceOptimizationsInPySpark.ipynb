{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b880347f-c49c-43b1-afc9-1b4a0c559fd6",
   "metadata": {},
   "source": [
    "# Performance Optimizations in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72d2050-8710-468a-a664-8b8b7d51bb62",
   "metadata": {},
   "source": [
    "## Caching and Persisting DataFrames\n",
    "\n",
    "- PySpark transformations are lazy, meaning they don't execute until an action is triggered. YOu can cache or persist DataFrames to avoid recomputing them multiple times\n",
    "- Scenarios: where transformations are applied multiple times\n",
    "\n",
    "### Key Storage Levels in PySpark:\n",
    "MEMORY_ONLY: Stores data in memory only (default for cache()).\n",
    "\n",
    "MEMORY_AND_DISK: Stores data in memory; if memory is insufficient, it spills over to disk.\n",
    "\n",
    "DISK_ONLY: Stores data only on disk.\n",
    "\n",
    "MEMORY_ONLY_SER: Stores data in memory as serialized objects (more efficient in terms of memory usage).\n",
    "\n",
    "MEMORY_AND_DISK_SER: Similar to MEMORY_ONLY_SER, but spills to disk when necessary.\n",
    "\n",
    "\n",
    "### When to Cache or Persist:\n",
    "Cache: When you are accessing the same DataFrame multiple times and the DataFrame is small enough to fit in memory.\n",
    "\n",
    "Persist: When you need more control over how the data is stored or if the DataFrame is too large for memory.\n",
    "\n",
    "### Important Considerations:\n",
    "Memory Management: Ensure that you have enough memory to cache/persist large datasets, as Spark will try to store them in memory, and may spill to disk if the memory is insufficient.\n",
    "\n",
    "Garbage Collection: Spark will clean up cached/persisted DataFrames when no longer needed, but you can manually uncache them using df.unpersist().\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad70b461-820e-4eab-b779-d0e254627037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/29 17:51:20 WARN CacheManager: Asked to cache already cached data.\n",
      "25/03/29 17:51:20 WARN CacheManager: Asked to cache already cached data.\n",
      "25/03/29 17:51:20 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+------+\n",
      "|employee_id|     name|department|salary|\n",
      "+-----------+---------+----------+------+\n",
      "|          1|     John|        HR| 55000|\n",
      "|          2|     Jane|   Finance| 80000|\n",
      "|          3|    James|        HR| 60000|\n",
      "|          4|     Anna|   Finance| 90000|\n",
      "|          5|      Bob| Marketing| 75000|\n",
      "|          6|    Emily| Marketing| 82000|\n",
      "|          7|    David|        HR| 65000|\n",
      "|          8|   George|   Finance| 95000|\n",
      "|          9|   Olivia| Marketing| 68000|\n",
      "|         10|     Liam|        HR| 54000|\n",
      "|         11|   Sophia|   Finance| 85000|\n",
      "|         12|    Lucas| Marketing| 78000|\n",
      "|         13| Isabella|   Finance| 92000|\n",
      "|         14|    Mason|        HR| 63000|\n",
      "|         15|   Amelia| Marketing| 79000|\n",
      "|         16|    Ethan|        HR| 67000|\n",
      "|         17|  Abigail|   Finance| 87000|\n",
      "|         18|    Aiden|        HR| 56000|\n",
      "|         19|Charlotte| Marketing| 81000|\n",
      "|         20|     Jack|        HR| 69000|\n",
      "+-----------+---------+----------+------+\n",
      "\n",
      "+-----------+---------+----------+------+----+\n",
      "|employee_id|     name|department|salary|rank|\n",
      "+-----------+---------+----------+------+----+\n",
      "|         10|     Liam|        HR| 54000|   1|\n",
      "|          1|     John|        HR| 55000|   2|\n",
      "|         18|    Aiden|        HR| 56000|   3|\n",
      "|          3|    James|        HR| 60000|   4|\n",
      "|         14|    Mason|        HR| 63000|   5|\n",
      "|          7|    David|        HR| 65000|   6|\n",
      "|         16|    Ethan|        HR| 67000|   7|\n",
      "|         20|     Jack|        HR| 69000|   8|\n",
      "|          2|     Jane|   Finance| 80000|   1|\n",
      "|         11|   Sophia|   Finance| 85000|   2|\n",
      "|         17|  Abigail|   Finance| 87000|   3|\n",
      "|          4|     Anna|   Finance| 90000|   4|\n",
      "|         13| Isabella|   Finance| 92000|   5|\n",
      "|          8|   George|   Finance| 95000|   6|\n",
      "|          9|   Olivia| Marketing| 68000|   1|\n",
      "|          5|      Bob| Marketing| 75000|   2|\n",
      "|         12|    Lucas| Marketing| 78000|   3|\n",
      "|         15|   Amelia| Marketing| 79000|   4|\n",
      "|         19|Charlotte| Marketing| 81000|   5|\n",
      "|          6|    Emily| Marketing| 82000|   6|\n",
      "+-----------+---------+----------+------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "spark = SparkSession.builder.appName('PerformanceTest').getOrCreate()\n",
    "\n",
    "df = spark.read.csv('./resources/7_employee.csv', header=True, inferSchema=True)\n",
    "# Cache the dataframe to avoid recomputing\n",
    "df.cache()\n",
    "\n",
    "# Optionally, you can persist it with a specific storage level:\n",
    "# eg: MEMORY_AND_DISK - data stored in memory and if not enough memory, spilled to disk,\n",
    "# DISK_ONLY - stores data only on disk, \n",
    "# MEMORY_ONLY - stores data only in memory\n",
    "# MEMORY_ONLY_SER - stores day in memory as serialized objects (more efficient in terms of memory usage)\n",
    "# MEMORY_AND_DISK_SER - similar to MEMORY_ONLY_SER, but spills to disk when necessary\n",
    "\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "df.show()\n",
    "\n",
    "window_specification = Window.partitionBy('department').orderBy(col('salary'))\n",
    "\n",
    "df_ranked = df.withColumn('rank', rank().over(window_specification))\n",
    "df_ranked.cache()\n",
    "df_ranked.show()\n",
    "df_ranked.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
