{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b9d8256-8f94-4283-a7e4-86bd1921bffa",
   "metadata": {},
   "source": [
    "# Loading the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9c845c-54e5-40fa-ac07-7ab41ee43312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/30 20:51:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/30 20:51:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+------------+-----------+\n",
      "|         customer_id|purchase_date|product_id|amount_spent|   location|\n",
      "+--------------------+-------------+----------+------------+-----------+\n",
      "|cec8ce71-f66c-405...|   2025-02-19|   prod_26|       86.82|    Phoenix|\n",
      "|9f64fb87-ba35-4d2...|   2024-11-30|   prod_75|      420.97|   New York|\n",
      "|fc370ee9-49a5-4c6...|   2024-10-12|   prod_33|      210.12|   New York|\n",
      "|f59526e3-961a-403...|   2024-09-24|   prod_92|      141.91|    Chicago|\n",
      "|530e6b90-7f0b-494...|   2023-04-11|   prod_53|      306.97|    Houston|\n",
      "|cf907b49-a479-454...|   2024-06-27|    prod_5|      447.03|    Chicago|\n",
      "|7be46435-3259-428...|   2024-09-13|   prod_54|      163.58|    Chicago|\n",
      "|381ff09f-13e4-4fe...|   2024-07-25|   prod_54|      201.32|    Chicago|\n",
      "|76629f06-a3e4-4c6...|   2023-12-19|   prod_65|      281.95|   New York|\n",
      "|fc9e962c-7337-40b...|   2024-01-11|   prod_69|      271.06|Los Angeles|\n",
      "+--------------------+-------------+----------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Set log level to ERROR (will suppress warnings)\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('CustomerTransactionCaseStudy').getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "df_large = spark.read.csv('./source/customer_transactions.csv', header=True, inferSchema=True)\n",
    "df_large.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae3133-5266-4d5b-8b1b-f09134e218aa",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "- check the schema and inspect some basic statistics to ensure the data is consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cedfc7eb-d434-4c83-91bd-b09f7cc2cabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- purchase_date: date (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- amount_spent: double (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|summary|         customer_id|      amount_spent|\n",
      "+-------+--------------------+------------------+\n",
      "|  count|             5000000|           5000000|\n",
      "|   mean|                NULL|254.97529644199867|\n",
      "| stddev|                NULL| 141.4397487898142|\n",
      "|    min|000019ea-d38b-42d...|              10.0|\n",
      "|    max|ffffed99-a43a-479...|             500.0|\n",
      "+-------+--------------------+------------------+\n",
      "\n",
      "+-----------+-------+\n",
      "|   location|  count|\n",
      "+-----------+-------+\n",
      "|    Phoenix| 999528|\n",
      "|Los Angeles|1000560|\n",
      "|    Chicago|1000170|\n",
      "|    Houston| 998853|\n",
      "|   New York|1000889|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_large.printSchema()\n",
    "df_large.select('customer_id', 'purchase_date', 'amount_spent').describe().show()\n",
    "\n",
    "#Check data distribution\n",
    "df_large.groupBy('location').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde59b71-8287-40f8-8ee8-d217d3202a8d",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "- Remove duplicates\n",
    "- Drop rows where amount_spent is missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35898c33-3a55-4a97-8949-59d0c2d49a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- purchase_date: date (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- amount_spent: double (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====================================>                      (5 + 3) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+------------+-----------+\n",
      "|         customer_id|purchase_date|product_id|amount_spent|   location|\n",
      "+--------------------+-------------+----------+------------+-----------+\n",
      "|6f8347e8-6b5b-493...|   2024-03-30|   prod_41|      229.62|    Phoenix|\n",
      "|dcec3b12-ee58-424...|   2024-03-28|   prod_70|      349.39|Los Angeles|\n",
      "|6e0f4818-9e7a-4bb...|   2024-06-27|   prod_84|        15.7|    Phoenix|\n",
      "|49f2d91e-0952-460...|   2025-02-12|   prod_48|       294.4|   New York|\n",
      "|5a46e3b7-dda1-4d4...|   2025-01-16|   prod_77|       256.9|    Houston|\n",
      "|f78de9d8-9bd5-412...|   2024-06-13|  prod_100|      349.53|Los Angeles|\n",
      "|c9191967-57b2-463...|   2024-05-26|    prod_5|      278.71|    Phoenix|\n",
      "|38bf8534-6232-459...|   2023-09-19|   prod_24|       12.65|Los Angeles|\n",
      "|c3c17991-4200-438...|   2024-08-25|   prod_64|      388.06|    Chicago|\n",
      "|4e5eb1e4-669e-48a...|   2023-12-12|   prod_89|      148.77|    Houston|\n",
      "+--------------------+-------------+----------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- purchase_date: date (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- amount_spent: double (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_clean = df_large.dropDuplicates()\n",
    "\n",
    "df_clean = df_clean.filter(df_clean.amount_spent.isNotNull())\n",
    "df_clean.printSchema()\n",
    "\n",
    "from pyspark.sql.functions import col, to_date\n",
    "df_clean = df_clean.withColumn('purchase_date', to_date(col('purchase_date'), 'yyyy-MM-dd'))\n",
    "df_clean.show(10)\n",
    "\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aef1b1-0bbe-49cb-8c82-4855c710db4a",
   "metadata": {},
   "source": [
    "# Data Aggregation\n",
    "- Perform some aggregations to compute key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d41512f4-0a62-4201-aeae-2e6acdd01ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:======>                                                   (1 + 8) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|         customer_id|total_spent|\n",
      "+--------------------+-----------+\n",
      "|7267fa3f-19dc-48e...|    1790.26|\n",
      "|3cf01753-5c92-40f...|    2566.46|\n",
      "|56f7f876-6bdc-44e...|    4374.74|\n",
      "|660f1afe-8254-4cf...|     1714.3|\n",
      "|964c2d71-21f0-4b9...|    3951.38|\n",
      "+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Execution Time : 5.79 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "start_time = time.time()\n",
    "customer_summary = df_clean.groupBy('customer_id').agg(\n",
    "    sum('amount_spent').alias('total_spent')\n",
    ")\n",
    "customer_summary.show(5)\n",
    "end_time = time.time()\n",
    "print(f'Execution Time : {end_time - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69a7c3-c27d-43c2-b9f5-0b946053893a",
   "metadata": {},
   "source": [
    "### Challenge: Large Aggregation Operations\n",
    "* Aggregations can cause shuffling of the data, which is expensive when working with large datasets\n",
    "* We can use partitioning or bucketing to minimize the shuffle during the aggregation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc998934-dea5-4a30-859a-970942bfaf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 8) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|         customer_id|       total_spent|\n",
      "+--------------------+------------------+\n",
      "|782d96d7-0e12-48e...|           2648.31|\n",
      "|4fe6223b-260b-40f...|           2133.73|\n",
      "|960ac431-6fc2-44e...|           2189.49|\n",
      "|7267fa3f-19dc-48e...|           1790.26|\n",
      "|3cf01753-5c92-40f...|2566.4600000000005|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Execution Time : 5.54 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# repartitioning the data into 100 partitions in a way that all rows with the same value for customer_id are likely to end up in the same partition\n",
    "df_clean_repartitioned = df_clean.repartition(100, 'customer_id') \n",
    "\n",
    "start_time = time.time()\n",
    "customer_summary = df_clean_repartitioned.groupBy('customer_id').agg(\n",
    "    sum('amount_spent').alias('total_spent')\n",
    ")\n",
    "customer_summary.show(5)\n",
    "end_time = time.time()\n",
    "print(f'Execution Time : {end_time - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1effa20-c6f5-4ff0-8728-6aaf9aa91358",
   "metadata": {},
   "source": [
    "# Identifying Top Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfef38f9-3372-42b1-8c90-2bd367b4607e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:===================>                                      (3 + 6) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|         customer_id|      total_spent|\n",
      "+--------------------+-----------------+\n",
      "|1cf7e2ba-ccfa-4c1...|8160.939999999998|\n",
      "|27e5457b-65ff-44d...|8114.209999999999|\n",
      "|0a185776-4a95-423...|          7557.26|\n",
      "|9c4ca216-540a-435...|          7484.98|\n",
      "|f9c8b5c3-055a-49f...|7409.340000000001|\n",
      "+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "top_customers = customer_summary.orderBy('total_spent', ascending=False).limit(10)\n",
    "top_customers.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f3b3fe-95f4-4083-8995-db2322cdf95c",
   "metadata": {},
   "source": [
    "# Joining with Product and Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ac5faa-03fd-435a-97a9-0a4afd3f389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 94:================================>                         (5 + 4) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------------+------------+--------+-------------+---------------+------+--------------+--------------------+---+-------------+\n",
      "|         customer_id|product_id|purchase_date|amount_spent|location| product_name|       category| price|          name|               email|age|         city|\n",
      "+--------------------+----------+-------------+------------+--------+-------------+---------------+------+--------------+--------------------+---+-------------+\n",
      "|0003b460-ca4e-4e6...|   prod_62|   2024-02-24|      130.11|New York|        Shoes|       Clothing|213.04|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_49|   2025-02-14|       465.9|New York|      Sweater|       Clothing|752.15|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_94|   2024-01-01|      288.45|New York|        Shoes|       Clothing| 44.21|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_74|   2025-02-18|      481.87| Houston|Fiction Novel|          Books|735.91|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_83|   2023-12-09|      390.85| Chicago| Refrigerator|Home Appliances|470.57|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_66|   2024-02-20|      369.79| Houston|     Cookbook|          Books| 390.8|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_19|   2024-08-21|       97.31| Houston|   Smartphone|    Electronics|813.75|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_65|   2024-07-20|        10.8| Houston|   Headphones|    Electronics|327.91|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_19|   2023-05-03|      422.76| Phoenix|   Smartphone|    Electronics|813.75|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_83|   2024-01-02|      257.74| Phoenix| Refrigerator|Home Appliances|470.57|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "+--------------------+----------+-------------+------------+--------+-------------+---------------+------+--------------+--------------------+---+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Execution time : 6.76 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import time\n",
    "product_info = spark.read.csv('./source/product_info.csv', header=True, inferSchema=True)\n",
    "customer_info = spark.read.csv('./source/customer_info.csv', header=True, inferSchema=True)\n",
    "\n",
    "start_time = time.time()\n",
    "df_joined = df_clean.join(product_info, \"product_id\")\n",
    "df_full = df_joined.join(customer_info, \"customer_id\")\n",
    "\n",
    "df_full.show(10)\n",
    "end_time = time.time()\n",
    "print(f\"Execution time : {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb00c22-4fe6-4a03-9daa-17737d39190d",
   "metadata": {},
   "source": [
    "# Optimizations during Join\n",
    "- Joins can cause shuffling and are often the slowest operation when processing large datasets. We can optimize joins by:\n",
    "\n",
    "  - Broadcasting the smaller `product_info` dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ce24383-770e-4eb4-9643-475dea472a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 107:======>                                                  (1 + 8) / 9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------------+------------+--------+-------------+---------------+------+--------------+--------------------+---+-------------+\n",
      "|         customer_id|product_id|purchase_date|amount_spent|location| product_name|       category| price|          name|               email|age|         city|\n",
      "+--------------------+----------+-------------+------------+--------+-------------+---------------+------+--------------+--------------------+---+-------------+\n",
      "|0003b460-ca4e-4e6...|   prod_62|   2024-02-24|      130.11|New York|        Shoes|       Clothing|213.04|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_49|   2025-02-14|       465.9|New York|      Sweater|       Clothing|752.15|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_94|   2024-01-01|      288.45|New York|        Shoes|       Clothing| 44.21|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_74|   2025-02-18|      481.87| Houston|Fiction Novel|          Books|735.91|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_83|   2023-12-09|      390.85| Chicago| Refrigerator|Home Appliances|470.57|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_66|   2024-02-20|      369.79| Houston|     Cookbook|          Books| 390.8|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_19|   2024-08-21|       97.31| Houston|   Smartphone|    Electronics|813.75|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_65|   2024-07-20|        10.8| Houston|   Headphones|    Electronics|327.91|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_19|   2023-05-03|      422.76| Phoenix|   Smartphone|    Electronics|813.75|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "|0003b460-ca4e-4e6...|   prod_83|   2024-01-02|      257.74| Phoenix| Refrigerator|Home Appliances|470.57|Henry Espinoza|scottshawn@exampl...| 27|East Jennifer|\n",
      "+--------------------+----------+-------------+------------+--------+-------------+---------------+------+--------------+--------------------+---+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Execution time : 6.41 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "product_info = spark.read.csv('./source/product_info.csv', header=True, inferSchema=True)\n",
    "customer_info = spark.read.csv('./source/customer_info.csv', header=True, inferSchema=True)\n",
    "\n",
    "start_time = time.time()\n",
    "df_joined = df_clean.join(broadcast(product_info), \"product_id\")\n",
    "df_full = df_joined.join(customer_info, \"customer_id\")\n",
    "\n",
    "df_full.show(10)\n",
    "end_time = time.time()\n",
    "print(f\"Execution time : {end_time - start_time :.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375591db-f781-43bd-884c-a976dd6a6b03",
   "metadata": {},
   "source": [
    "# Writing Results\n",
    "- After performing all the necessary transformations, write the results back to storage.\n",
    "- It's critical to optimize the write process to avoid generating too many small files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c8bd135-c87d-4c3a-a6d6-53f49db08f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_full.coalesce(1).write.mode('overwrite').parquet('./target/fully_joined_dataset.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e5901b-a9cf-4f07-bb72-5a635be99ba0",
   "metadata": {},
   "source": [
    "# Monitoring and Profiling\n",
    "\n",
    "#### Understanding Plan\n",
    "Parsed Logical Plan - shows the SQL-like plan before optimization; Displays operations like JOIN, FILTER, PROJECT, AGGREGATE\n",
    "\n",
    "Analyzed Logical Plan - checks column names and types; Ensures column `customer_id` is valid and `amount_spent` is `double`, etc\n",
    "\n",
    "Optimized Logical Plan - applies optimizations like: Predicate pushdown (filters only), Broadcast join (small tables sent to all workers), Column pruning (removes unused columns)\n",
    "\n",
    "Physical Plan - Spark physically executes the query. Uses: BroadcastHashJoin, SortMergeJoin, Exchange operations\n",
    "    - Displays `partitioning strategy` and `file scan details`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9edfc247-b54d-4ef7-8cad-e1341ed414a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [customer_id])\n",
      ":- Project [product_id#19, customer_id#17, purchase_date#219, amount_spent#20, location#21, product_name#1035, category#1036, price#1037]\n",
      ":  +- Join Inner, (product_id#19 = product_id#1034)\n",
      ":     :- Project [customer_id#17, to_date(purchase_date#18, Some(yyyy-MM-dd), Some(America/Los_Angeles), false) AS purchase_date#219, product_id#19, amount_spent#20, location#21]\n",
      ":     :  +- Filter isnotnull(amount_spent#20)\n",
      ":     :     +- Deduplicate [purchase_date#18, location#21, customer_id#17, amount_spent#20, product_id#19]\n",
      ":     :        +- Relation [customer_id#17,purchase_date#18,product_id#19,amount_spent#20,location#21] csv\n",
      ":     +- ResolvedHint (strategy=broadcast)\n",
      ":        +- Relation [product_id#1034,product_name#1035,category#1036,price#1037] csv\n",
      "+- Relation [customer_id#1059,name#1060,email#1061,age#1062,city#1063] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "customer_id: string, product_id: string, purchase_date: date, amount_spent: double, location: string, product_name: string, category: string, price: double, name: string, email: string, age: int, city: string\n",
      "Project [customer_id#17, product_id#19, purchase_date#219, amount_spent#20, location#21, product_name#1035, category#1036, price#1037, name#1060, email#1061, age#1062, city#1063]\n",
      "+- Join Inner, (customer_id#17 = customer_id#1059)\n",
      "   :- Project [product_id#19, customer_id#17, purchase_date#219, amount_spent#20, location#21, product_name#1035, category#1036, price#1037]\n",
      "   :  +- Join Inner, (product_id#19 = product_id#1034)\n",
      "   :     :- Project [customer_id#17, to_date(purchase_date#18, Some(yyyy-MM-dd), Some(America/Los_Angeles), false) AS purchase_date#219, product_id#19, amount_spent#20, location#21]\n",
      "   :     :  +- Filter isnotnull(amount_spent#20)\n",
      "   :     :     +- Deduplicate [purchase_date#18, location#21, customer_id#17, amount_spent#20, product_id#19]\n",
      "   :     :        +- Relation [customer_id#17,purchase_date#18,product_id#19,amount_spent#20,location#21] csv\n",
      "   :     +- ResolvedHint (strategy=broadcast)\n",
      "   :        +- Relation [product_id#1034,product_name#1035,category#1036,price#1037] csv\n",
      "   +- Relation [customer_id#1059,name#1060,email#1061,age#1062,city#1063] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [customer_id#17, product_id#19, purchase_date#219, amount_spent#20, location#21, product_name#1035, category#1036, price#1037, name#1060, email#1061, age#1062, city#1063]\n",
      "+- Join Inner, (customer_id#17 = customer_id#1059)\n",
      "   :- Project [product_id#19, customer_id#17, purchase_date#219, amount_spent#20, location#21, product_name#1035, category#1036, price#1037]\n",
      "   :  +- Join Inner, (product_id#19 = product_id#1034), rightHint=(strategy=broadcast)\n",
      "   :     :- Aggregate [purchase_date#18, location#21, customer_id#17, amount_spent#20, product_id#19], [customer_id#17, cast(gettimestamp(purchase_date#18, yyyy-MM-dd, TimestampType, Some(America/Los_Angeles), false) as date) AS purchase_date#219, product_id#19, amount_spent#20, location#21]\n",
      "   :     :  +- Filter (isnotnull(amount_spent#20) AND (isnotnull(product_id#19) AND isnotnull(customer_id#17)))\n",
      "   :     :     +- Relation [customer_id#17,purchase_date#18,product_id#19,amount_spent#20,location#21] csv\n",
      "   :     +- Filter isnotnull(product_id#1034)\n",
      "   :        +- Relation [product_id#1034,product_name#1035,category#1036,price#1037] csv\n",
      "   +- Filter isnotnull(customer_id#1059)\n",
      "      +- Relation [customer_id#1059,name#1060,email#1061,age#1062,city#1063] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [customer_id#17, product_id#19, purchase_date#219, amount_spent#20, location#21, product_name#1035, category#1036, price#1037, name#1060, email#1061, age#1062, city#1063]\n",
      "   +- SortMergeJoin [customer_id#17], [customer_id#1059], Inner\n",
      "      :- Sort [customer_id#17 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(customer_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=2785]\n",
      "      :     +- Project [product_id#19, customer_id#17, purchase_date#219, amount_spent#20, location#21, product_name#1035, category#1036, price#1037]\n",
      "      :        +- BroadcastHashJoin [product_id#19], [product_id#1034], Inner, BuildRight, false\n",
      "      :           :- HashAggregate(keys=[purchase_date#18, location#21, customer_id#17, amount_spent#20, product_id#19], functions=[], output=[customer_id#17, purchase_date#219, product_id#19, amount_spent#20, location#21])\n",
      "      :           :  +- Exchange hashpartitioning(purchase_date#18, location#21, customer_id#17, amount_spent#20, product_id#19, 200), ENSURE_REQUIREMENTS, [plan_id=2777]\n",
      "      :           :     +- HashAggregate(keys=[purchase_date#18, location#21, customer_id#17, knownfloatingpointnormalized(normalizenanandzero(amount_spent#20)) AS amount_spent#20, product_id#19], functions=[], output=[purchase_date#18, location#21, customer_id#17, amount_spent#20, product_id#19])\n",
      "      :           :        +- Filter ((isnotnull(amount_spent#20) AND isnotnull(product_id#19)) AND isnotnull(customer_id#17))\n",
      "      :           :           +- FileScan csv [customer_id#17,purchase_date#18,product_id#19,amount_spent#20,location#21] Batched: false, DataFilters: [isnotnull(amount_spent#20), isnotnull(product_id#19), isnotnull(customer_id#17)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(amount_spent), IsNotNull(product_id), IsNotNull(customer_id)], ReadSchema: struct<customer_id:string,purchase_date:date,product_id:string,amount_spent:double,location:string>\n",
      "      :           +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=2780]\n",
      "      :              +- Filter isnotnull(product_id#1034)\n",
      "      :                 +- FileScan csv [product_id#1034,product_name#1035,category#1036,price#1037] Batched: false, DataFilters: [isnotnull(product_id#1034)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:string,product_name:string,category:string,price:double>\n",
      "      +- Sort [customer_id#1059 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(customer_id#1059, 200), ENSURE_REQUIREMENTS, [plan_id=2786]\n",
      "            +- Filter isnotnull(customer_id#1059)\n",
      "               +- FileScan csv [customer_id#1059,name#1060,email#1061,age#1062,city#1063] Batched: false, DataFilters: [isnotnull(customer_id#1059)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/kushal/Documents/Projects/Python/Python_Revision/98. PySpa..., PartitionFilters: [], PushedFilters: [IsNotNull(customer_id)], ReadSchema: struct<customer_id:string,name:string,email:string,age:int,city:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_full.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a32a57e-225b-475a-8018-04a04ab289b8",
   "metadata": {},
   "source": [
    "# SUMMARY\n",
    "\n",
    "### Challenges Faced:\n",
    "Data Shuffling: Shuffling happens during operations like groupBy and join. This can cause performance bottlenecks. Optimization: Partitioning the data appropriately and using broadcast joins for smaller datasets.\n",
    "\n",
    "Memory Usage: Large aggregations can overwhelm memory. Optimization: Use partitioning and caching appropriately to ensure data fits within the memory.\n",
    "\n",
    "File Handling: Writing large datasets may result in too many small files. Optimization: Use .coalesce() to reduce the number of output files.\n",
    "\n",
    "\n",
    "\n",
    "### In this case study, we:\n",
    "\n",
    "Loaded and cleaned large datasets.\n",
    "\n",
    "Aggregated the data efficiently using partitioning.\n",
    "\n",
    "Performed joins using broadcast strategies.\n",
    "\n",
    "Written optimized results back to storage in a performant way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
